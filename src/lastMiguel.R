#ANALYSIS EXPLORATORY AND PRE-PROCESSING
data<- train10.df

names(data)[10001] <- "clazz" # change the factor class to clazz
matrixdata1<-as.matrix(data[-10001])
class(matrixdata1)
dim(matrixdata1)
library(tm)
library(wordcloud)

N<-nrow(data)
ntrain <- round(N*0.8)     # number of training examples
tr <- sample(N,ntrain) # indices of training samples

v <- sort(colSums(matrixdata1),decreasing=TRUE)
plot(log(v))
d <- data.frame(word = colnames(data[,-10001]),freq=v/sum(v))
wordcloud(d$word,d$freq,scale=c(8,.5),max.words=100, random.order=FALSE)
plot(data[,10000],data[,10001])


#source('C:/Users/miguelmossa/Documents/Machine Learning - MIRI/L1pack-II/reuters-aux.R')
# Now we weight by inverse document frequency
data.tfidf1 <- tfidf.weight(matrixdata1)
# and normalize by vector length
data.tfidf1 <- div.by.euc.length(data.tfidf1)

dim(data.tfidf1)
summary(colSums(data.tfidf1))

#1. remove those words shorter than 3 characters

data.tfidf.3 <- subset.data.frame (data.tfidf1, 
                                   select=sapply(colnames(data.tfidf1), FUN=nchar)>2)

dim(data.tfidf.3)

#2. remove those words whose total sum is not greater than the third quartile of the distribution

(r.3rdQ <- summary(colSums(data.tfidf.3))[5])

data.tfidf.3 <- subset.data.frame (data.tfidf.3, 
                                   select=colSums(data.tfidf.3)>r.3rdQ)
dim(data.tfidf.3)

#cambio de nombre a la columna de clases
target.class1<-data$clazz

#cambio de nombre ... a points3
colnames(data.tfidf.3)[colnames(data.tfidf.3) %in% c("...")]<-"points3"

#asignacion de columna clases a matriz de frequencia inversa filtrada
datacomplete<-data.frame(data.tfidf.3,target.class1)


# FEATURE SELECTION AND EXTRACTION

#-----------PCA-------------------------------

library(FactoMineR)
PCAnew <- PCA(datacomplete,quali.sup=c(ncol(datacomplete)),ncp=500) # retain the first 500 PCs
summary(PCAnew)
plot(PCAnew$eig$eigenvalue,type="l",main="Screeplot")
#Psiframe<-data.frame(target.class1,PCAnew$ind$coord[,1:500])
Psiframe<-data.frame(target.class1,PCAnew$ind$coord)


#-----------Relief ----------------------------
library(CORElearn)
estReliefF <- attrEval("target.class1",datacomplete, estimator="ReliefFexpRank", ReliefIterations=500)
featuresranking<-sort(estReliefF,decreasing=TRUE)



# MODELS  

library(kernlab)

K=c(10,30,50,70,100,200,300,380,500) # dim size to use CV
Cp=10^seq(-2,3) # list of C's possible parameters


#---------SVM and PCA--------------------------

# initialize vectors to show the results
err <- numeric(length(Cp))
Cs<-numeric(length(K))
error.rate.test.Ondim<-numeric(length(K))
error.CV<-numeric(length(K))
SV<-numeric(length(K))


for(i in 1:length(K)){
  
  for(j in 1:length(Cp)){
    mi.svm <- ksvm (target.class1~.,data=Psiframe[tr,1:K[i]],C=Cp[j],cross=10)
    err[j]<-cross(mi.svm)
  }
  Cmin<-which.min(err)
  Cs[i]<-Cp[Cmin]
  error.CV[i]<-min(err)
  mi.svm.final <- ksvm (target.class1~.,data=Psiframe[tr,1:K[i]],C=Cp[Cmin],cross=10)
  
  SV[i]<-nSV(mi.svm.final) # Support vectors
  
  svmpred <- predict (mi.svm.final, Psiframe[-tr,2:K[i]])
  
  tt <- table(datacomplete[-tr,ncol(datacomplete)],svmpred)
  
  error_rate.test <- 100*(1-sum(diag(tt))/sum(tt))
  print(error_rate.test)
  error.rate.test.Ondim[i]<-error_rate.test  
}

#---------SVM and Relief-------------------------------------------

# initialize vectors to show the results
err2 <- numeric(length(Cp))
Cs2<-numeric(length(K))
error.rate.test.Ondim2<-numeric(length(K))
error.CV2<-numeric(length(K))
SV2<-numeric(length(K))

for(i in 1:length(K)){
  x<-cbind(datacomplete$target.class1,datacomplete[,names(datacomplete) %in% names(featuresranking)[1:K[i]]])
  colnames(x)[1] <- "target.class1"
  
  #Calculate the best C parameter
  for(j in 1:length(Cp)){
    mi.svm2 <- ksvm (target.class1~.,data=x[tr,],C=Cp[j],cross=10)
    err2[j]<-cross(mi.svm2)
  }
  Cmin<-which.min(err2)
  Cs2[i]<-Cp[Cmin] # the best C
  error.CV2[i]<-min(err2) # CV error with the best C
  
  mi.svm.final2 <- ksvm (target.class1~.,data=x[tr,],C=Cp[Cmin],cross=10) # recalculate the model using the best C parameter
  
  SV2[i]<-nSV(mi.svm.final2) # Support vectors
  
  svmpred2 <- predict (mi.svm.final2, datacomplete[-tr,-ncol(datacomplete)])
  
  tt <- table(datacomplete[-tr,ncol(datacomplete)],svmpred2)
  
  error_rate.test <- 100*(1-sum(diag(tt))/sum(tt))
  print(error_rate.test)
  error.rate.test.Ondim2[i]<-error_rate.test  
}
----------------------------------------------------------------------
library(psych)
harmDiagProb <-function(x){
  var=1
  i=0
  for(i in i:nrow(x) )
    var[i] <- prop.table(x,1)[i,i]
  return(as.numeric(var))
}




library(randomForest)
model.rf <- randomForest(target.class1 ~ ., data = x[tr,], ntree=100, proximity=FALSE, strata=tess$V601)
summary(x[tr,])
summary(x)
################################################################################################################################################
#---------RandomForest and PCA--------------------------
################################################################################################################################################
# initialize vectors to show the results
err3 <- numeric(length(Cp))
Cs3<-numeric(length(K))
error.rate.test.Ondim3<-numeric(length(K))
error.rate.harmonic3<-numeric(length(K))
error.CV3<-numeric(length(K))
#SV<-numeric(length(K))
K=c(10,30,50,70,100,200,300,380,500) # dim size to use CV
(ntrees <- round(10^seq(1,4,by=0.4))) # ntrees posibilities


for(i in 1:length(K)){
  for(j in 1:length(ntrees)){
    #mi.svm <- ksvm (target.class1~.,data=Psiframe[tr,1:K[i]],C=Cp[j],cross=10)
    mi.rf <- randomForest(target.class1 ~ ., data = Psiframe[tr,1:K[i]], ntree=ntrees[j], proximity=FALSE, strata=x$target.class1)
    err3[j]<- mi.rf$err.rate[ntrees[j],1]
  }
  nTreemin <- which.min(err3)
  Cs3[i]<-ntrees[nTreemin]
  error.CV3[i]<-min(err3)
  mi.rf.final <- randomForest(target.class1 ~ ., data = Psiframe[tr,1:K[i]], ntree=ntrees[nTreemin], proximity=FALSE, strata=x$target.class1)
  #mi.svm.final <- ksvm (target.class1~.,data=Psiframe[tr,1:K[i]],C=Cp[Cmin],cross=10)
  
  #SV[i]<-nSV(mi.svm.final) # Support vectors
  
  #svmpred <- predict (mi.svm.final, Psiframe[-tr,2:K[i]])
  pred.rf <- predict (mi.rf.final, Psiframe[-tr,2:K[i]], type="class")
  
  (ct <- table(Truth=datacomplete[-tr,ncol(datacomplete)], Pred=pred.rf))
  #tt <- table(datacomplete[-tr,ncol(datacomplete)],svmpred)
  


  error_rate.test <- 100*(1-sum(diag(ct))/sum(ct))
  print(error_rate.test)
  error.rate.test.Ondim3[i]<-error_rate.test  
  error.rate.harmonic3[i]-harmonic.mean(harmDiagProb(ct)*100)
}


################################################################################################################################################
#---------RandomForest and Relief-------------------------------------------
################################################################################################################################################
# initialize vectors to show the results
err4 <- numeric(length(Cp))
Cs4<-numeric(length(K))
error.rate.test.Ondim4<-numeric(length(K))

error.rate.harmonic4<-numeric(length(K))
error.CV4<-numeric(length(K))
#SV2<-numeric(length(K))

for(i in 1:length(K)){
  x<-cbind(datacomplete$target.class1,datacomplete[,names(datacomplete) %in% names(featuresranking)[1:K[i]]])
  colnames(x)[1] <- "target.class1"
  #Calculate the best C parameter
  for(j in 1:length(ntrees)){
    #mi.svm2 <- ksvm (target.class1~.,data=x[tr,],C=Cp[j],cross=10)11
    mi.rf <- randomForest(target.class1 ~ ., data = x[tr,], ntree=ntrees[j], proximity=FALSE, strata=x$target.class1)
    err4[j]<- mi.rf$err.rate[ntrees[j],1]
    #err2[j]<-cross(mi.svm2)
  }
  nTreemin <- which.min(err4)
  Cs4[i]<-ntrees[nTreemin] # the best C
  error.CV4[i]<-min(err4) # CV error with the best C

  mi.rf.final2 <- randomForest(target.class1 ~ ., data = x[tr,], ntree=ntrees[nTreemin], proximity=FALSE, strata=x$target.class1)
  #mi.svm.final2 <- ksvm (target.class1~.,data=x[tr,],C=Cp[Cmin],cross=10) # recalculate the model using the best C parameter
  
  #SV2[i]<-nSV(mi.svm.final2) # Support vectors
  
  pred.rf2 <- predict (mi.rf.final2, x[-tr,], type="class")
  
  (ct <- table(Truth=datacomplete[-tr,ncol(datacomplete)], Pred=pred.rf2))
  
  #svmpred2 <- predict (mi.svm.final2, datacomplete[-tr,-ncol(datacomplete)])
  
  #tt <- table(datacomplete[-tr,ncol(datacomplete)],svmpred2)
  
  error_rate.test <- 100*(1-sum(diag(ct))/sum(ct))
  print(error_rate.test)
  error.rate.test.Ondim4[i]<-error_rate.test  
  error.rate.harmonic4[i] <- harmonic.mean(harmDiagProb(ct))
  
}
----------------------------------------------------------------------  

  # the results using SVM with PCA and Relief
  
Testing.PCA.SVM<-cbind(Cs,SV,error.rate.test.Ondim,error.CV,K)
colnames(Testing.PCA.SVM)<-c("OptC-SVM","SupportVectors","ErrorRateTest","CVerror","Dim(PCs)")
Testing.Relief.SVM<-cbind(Cs2,SV2,error.rate.test.Ondim2,error.CV2,K)
colnames(Testing.Relief.SVM)<-c("OptC-SVM","SupportVectors","ErrorRateTest","CVerror","Dim(RankingRelief)")


Testing.PCA.rf<-cbind(Cs3,error.rate.test.Ondim3,error.CV3,K)
colnames(Testing.PCA.rf)<-c("OptN-trees","ErrorRateTest","OOBError", "Dim(PCs)")

Testing.Relief.rf<-cbind(Cs4,error.rate.test.Ondim4,error.CV4,K)
colnames(Testing.Relief.rf)<-c("OptN-trees","ErrorRateTest","OOBError","Dim(RankingRelief)")

Testing.PCA.SVM
Testing.Relief.SVM
library(xtable)

import 
print(xtable(Testing.PCA.rf))

print(xtable(Testing.Relief.rf))
